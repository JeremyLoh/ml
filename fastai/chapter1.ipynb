{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-07T12:43:25.098672700Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('C:/Users/jerem/.fastai/data/oxford-iiit-pet/annotations'), Path('C:/Users/jerem/.fastai/data/oxford-iiit-pet/images')]\n",
      "7390\n",
      "C:\\Users\\jerem\\.fastai\\data\\oxford-iiit-pet\\images\\Abyssinian_1.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.134439</td>\n",
       "      <td>0.029362</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='22' class='' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      23.91% [22/92 00:31&lt;01:40 0.0413]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# running first notebook\n",
    "# https://docs.fast.ai/tutorial.vision.html\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# https://nbviewer.org/github/fastai/fastbook/blob/master/01_intro.ipynb\n",
    "# dataset called Oxfort-IIIT Pet Dataset, 7349 images of cats and dogs form 37 different breeds\n",
    "path = untar_data(URLs.PETS)\n",
    "\n",
    "# path = C:\\Users\\jerem\\.fastai\\data\\oxford-iiit-pet\n",
    "# path.ls() = [Path('C:/Users/jerem/.fastai/data/oxford-iiit-pet/annotations'), Path('C:/Users/jerem/.fastai/data/oxford-iiit-pet/images')]\n",
    "print(path.ls())\n",
    "\n",
    "# get_image_files helps us get all images recursively in one folder\n",
    "files = get_image_files(path/\"images\")\n",
    "print(len(files))\n",
    "print(files[0])\n",
    "\n",
    "# based on filenames, cats have capital first letter, dogs have lowercase first letter\n",
    "def is_cat(filename):\n",
    "    return filename[0].isupper()\n",
    "\n",
    "# dataloaders, we need this for feeding data to model\n",
    "# item_tfms is a Transform applied to all items of dataset\n",
    "# Resize(224) will resize image to 224 by 224, using a random crop on the largest dimension to make it a square\n",
    "# so that we can batch items together\n",
    "dls = ImageDataLoaders.from_name_func(path, fnames=files, valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224))\n",
    "\n",
    "# check if dataloaders look okay\n",
    "dls.show_batch()\n",
    "\n",
    "# pretrained model will be used, it will be fine-tuned using transfer learning, to create a model specially for recognizing dogs and cats\n",
    "\n",
    "# Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to C:\\Users\\jerem/.cache\\torch\\hub\\checkpoints\\resnet34-b627a593.pth\n",
    "# 100%|██████████| 83.3M/83.3M [00:01<00:00, 78.7MB/s]\n",
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)\n",
    "\n",
    "# epoch \ttrain_loss \tvalid_loss \terror_rate \ttime\n",
    "# 0 \t0.134439 \t0.029362 \t0.008119 \t01:16\n",
    "# epoch \ttrain_loss \tvalid_loss \terror_rate \ttime\n",
    "# 0 \t0.052154 \t0.020385 \t0.006089 \t02:17\n",
    "# error rate is the proportion of images that were incorrectly identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f20007461502704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T12:43:18.851045100Z",
     "start_time": "2024-01-07T12:43:17.281671200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FileUpload(value=(), description='Upload')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a2372ed41f14dc8ae9a5abb2fe4ee5c"
      }
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "uploader = widgets.FileUpload()\n",
    "uploader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T14:03:46.875055200Z",
     "start_time": "2024-01-07T14:03:46.765901900Z"
    }
   },
   "id": "52cfc0cb3d3d203f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileUpload(value=(), description='Upload')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FileUpload' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIs this a cat?: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mis_cat\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProbability it\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms a cat: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprobs[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43muploader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m     13\u001B[0m     is_cat_image(uploader\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'FileUpload' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import PILImage\n",
    "\n",
    "print(uploader)\n",
    "\n",
    "def is_cat_image(image_data):\n",
    "    upload_img = PILImage.create(image_data)\n",
    "    show_image(upload_img.to_thumb(224))\n",
    "    is_cat, _, probs = learn.predict(upload_img)\n",
    "    print(f\"Is this a cat?: {is_cat}.\")\n",
    "    print(f\"Probability it's a cat: {probs[1].item():.6f}\")\n",
    "\n",
    "if uploader.data[0]:\n",
    "    is_cat_image(uploader.data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T14:03:57.139418900Z",
     "start_time": "2024-01-07T14:03:51.305920900Z"
    }
   },
   "id": "5810dd8260930047",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What is a neural network?\n",
    "\n",
    "We would like some function that is so flexible that it could be used to solve any given problem, just by varying its weights\n",
    "- this is the neural network\n",
    "- based on mathematical proof called universal approximation theorem\n",
    "\n",
    "We can focus on finding good weight assignments (training the neural network)\n",
    "\n",
    "We also need a completely general way to update the weights of a neural network to make it improve at any given task\n",
    "- Stochastic gradient descent (SGD)\n",
    "- Universal approximation theorem\n",
    "\n",
    "To recap\n",
    "- neural network is a particular kind of machine learning model\n",
    "- neural networks are special because they are highly flexible, can solve wide range of problems by just finding the correct weights\n",
    "- Stochastic gradient descent (SGD) helps us to find those weight values automatically"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3780f12cbff449e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Limitations inherent to machine learning\n",
    "- a model cannot be created without data\n",
    "- a model can only learn to operate on the patterns seen in the input data used to train it\n",
    "- this learning approach only creates predictions, not recommended actions\n",
    "- not enough to just have examples of input data, we need labels for that data also. (e.g. picture of cats and dogs are not enough to train the model, we need a label for each picture, saying which one are dogs, and which are cats)\n",
    "\n",
    "Generally, most organizations say they don't have enough data, what they mean is labelled data\n",
    "\n",
    "Predictions - might be recommending things user might already know, for recommendation systems\n",
    "\n",
    "How a model interacts with its environment might create feedback loops\n",
    "- predictive policing model based on where arrests have been made in the past. Not actually predicting crime, but rather predicting arrests, reflecting biases in existing policing processes\n",
    "- Law enforcement officers might use this model to decide where to focus their police activity, resulting in increased arrests in those areas\n",
    "- Data on these additional arrests would be fed back to retrain future versions of the model\n",
    "\n",
    "This is a positive feedback loop, where the more the model is used, the more biased the data becomes, making the model more biased and so forth"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eddd12fe85df221"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How our image recognizer works\n",
    "https://nbviewer.org/github/fastai/fastbook/blob/master/01_intro.ipynb\n",
    "\n",
    "```python\n",
    "from fastai.vision.all import *\n",
    "```\n",
    "- Give us all the functions and classes that we need to create a wide variety of computer vision models\n",
    "- A lot of coders recommend avoiding importing a whole library like this (import *) because it can cause problems in large software projects\n",
    "- For interactive work in Jupyter notebook, it works great\n",
    "\n",
    "```python\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "```\n",
    "- download a standard dataset from the fast.ai datasets collection (if not previously downloaded) and extracts it (if not previously extracted), and returns a Path object with the extracted location\n",
    "- https://docs.fast.ai/data.external.html#datasets\n",
    "\n",
    "```python\n",
    "def is_cat(filename):\n",
    "    return filename[0].isupper()\n",
    "```\n",
    "- we define a function that labels cats based on a filename rule provided by the dataset creators\n",
    "\n",
    "```python\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224)\n",
    ")\n",
    "```\n",
    "- Tells fastai what kind of dataset we have and how it is structured\n",
    "- First part of the class name will be generally the type of data you have (e.g. image or text)\n",
    "- We need to tell fastai how to get the labels from the dataset. Computer vision datasets are normally structured in a way that the label for an image is part of the filename, or path - commonly the parent folder name\n",
    "- Define the Transform that we need. A transform contains code that is applied automatically during training. item_tfms are applied to each item, while batch_tfms are applied to a batch of items at a time using the GPU, so they are particularly fast\n",
    "    - Why Resize(224) for the item_tfms? It is the standard size for historical reasons. If you increase the size, you often get a model that can have better results (it can focus on more details), but at the price of speed and memory consumption. Opposite is true if you decrease the size    \n",
    "-  Classification model is one that attempts to predict a class, or category (predicting from a number of discrete possibilities, e.g. cat or dog\n",
    "- Regression model is one that attempts to predict one or more numeric quantities such as temperature or a location\n",
    "- from_name_func method tells fastai that the label can be extracted using a function applied to the filename\n",
    "- valid_pct=0.2 is the most important parameter. Tells fastai to hold out 20% of data and not use it for training the model at all. This is the validation set. The remaining 80% is the training set.\n",
    "    - The validation set is used to measure the accuracy of the model\n",
    "    - By default, the 20% validation set is selected randomly\n",
    "        - The parameter \"seed=42\" sets the random seed to the same value, so that we get the same validation set. If we change our model and retrain it with this same seed, we know any differences are due to changes in model and not due to the random validation set\n",
    "- fastai will ALWAYS show you the model's accuracy using the validation set, NEVER the training set. This is critical, if you train a large enough model for a long enough time, it eventually memorize the label of every item in your dataset. We only care about how well our model works on previously unseen images \n",
    "- During training, the longer you train for, the better the accuracy on the training set and the validation set, but eventually, it will start to get worse as the model starts to memorize the training set rather than finding generalizable underlying patterns in the data\n",
    "    - This is when the model is overfitting \n",
    "- Overfitting is the single most important and challenging issue when training for all machine learning practitioners, and all algorithms \n",
    "    - Easy to create a model that does a great job on the data that it is trained on, but much harder to make accurate predictions on the data the model has never seen before\n",
    "    - Overfitting -> when validation accuracy is getting worse during training \n",
    "        - Often see practitioners using over-fitting avoidance techniques even when they have enough data that they didn't need to do so, ending up with a model that may be less accurate than what they could have achieved.\n",
    "\n",
    " important: Validation Set: When you train a model, you must always have both a training set and a validation set, and must measure the accuracy of your model only on the validation set. If you train for too long, with not enough data, you will see the accuracy of your model start to get worse; this is called overfitting. fastai defaults valid_pct to 0.2, so even if you forget, fastai will create a validation set for you!\n",
    " \n",
    "```python\n",
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "```\n",
    "- Tell fastai to create a convolutional neural network (CNN) and specify which architecture to use (i.e what kind of model to create), what data we want to train it on, what metric to use\n",
    "- CNN iss a current state of the art approach to create computer vision models. inspired by the way human vision system works\n",
    "- Resnet34, 34 refers to the number of layers in this variant of the architecture (other options are 18, 50, 101, 152).\n",
    "- Models using more layers take longer to train, and are more prone to overfitting (i.e. you can't train them for as many epochs before the accuracy on the validation set starts to get worse). When using more data, they can be quite a bit more accurate (more layers)\n",
    "\n",
    "## What is a metric?\n",
    "- Metric is a function that measures the quality of the model's predictions using the validation set. Will be printed at the end of each epoch\n",
    "- In this case, we are using error_rate, provided function by fastai, that tells what percentage of images in the validation set are being classified incorrectly.\n",
    "- Another common metric is accuracy, which is just 1.0 - error_rate\n",
    "- Concept of metric might remind you of loss, but there is an important distinction. Entire purpose of loss is to define a \"measure of performance\" that the training system can use to update weights automatically. A good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand, and that hews as closely as possible to what you want the model to do. At times, you might decide that the loss function is a suitable metric, but that is not necessarily the case\n",
    "\n",
    "`vision_learner` has a parameter `pretrained` that defaults to `True`, which sets weights in your model to values that have been recognized by experts to recognize a thousand different categories across 1.3 million photos (using ImageNet dataset).\n",
    "- A model that has weights that have already been trained on some other dataset is called a pretrained model\n",
    "- You should nearly always use a pretrained model, it means that your model, is already very capable (even before showing it any of your data)\n",
    "- When using a pretrained model, `vision_learner` will remove the last layer, since that is always customized to the original training task, and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the `head`\n",
    "\n",
    "Using a pretrained model for a task different from what it was originally trained for is known as `transfer learning`\n",
    "\n",
    "Sixth line of code tells fastai how to fit the model\n",
    "```python\n",
    "learn.fine_tune(1)\n",
    "```\n",
    "- The architecture only describes a template for a mathematical function, it doesn't actually do anything until we provide values for the millions of parameters it contains\n",
    "- Key to deep learning - determine how to fit the parameters of a model to get it to solve your problem\n",
    "- In order to fit a model, we have to provide at least one piece of info, how many times to look at each image (known as number of epochs). Number you select will depend on how much time you have available, and how long you find it takes in practice to fit your model. If you selected a too small number, you can always train for more epochs later\n",
    "- Why is it called fine_tune and not fit? There is a fit method that fits a model (i.e look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we started with a pretrained model, and we don't want to throw away all those capabilities that it already has\n",
    "\n",
    "Fine-tuning: A transfer learning technique where the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining\n",
    "\n",
    "When you use fine_tune method, fastai will use these tricks for you\n",
    "There are a few parameters you can set, which we will discuss later, but in the default form shown, it does two steps:\n",
    "1) Use one epoch to fit just those parts of the model necessary to get the new random head (last layer) to work correctly with your dataset\n",
    "2) Use the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (generally don't require many changes from the pretrained weights)\n",
    "\n",
    "The head of a model is the part that is newly added to be specific to the new dataset\n",
    "\n",
    "An epoch is one complete pass through the dataset\n",
    "\n",
    "After calling `fit`, the results after each epoch are printed, showing the epoch number, the training and validation set losses (\"measure of performance\" used for training the model), and any metrics you have requested (error_rate in this case)\n",
    "\n",
    "Suggests a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.\n",
    "\n",
    "In general, you'll find that a small number of general approaches in deep learning can go a long way, if you're a bit creative in how you represent your data! You shouldn't think of approaches like the ones described here as \"hacky workarounds,\" because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.\n",
    "\n",
    "`fit_one_cycle` is the most common method for training fastai models from scratch (without transfer learning)\n",
    "- e.g. used in tabular data, where there isn't really a pretrained model available for the task\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17b1fa47c0e9a977"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sidebar: Datasets: Food for Models\n",
    "\n",
    "You’ve already seen quite a few models in this section, each one trained using a different dataset to do a different task. In machine learning and deep learning, we can’t do anything without data. So, the people that create datasets for us to train our models on are the (often underappreciated) heroes. Some of the most useful and important datasets are those that become important academic baselines; that is, datasets that are widely studied by researchers and used to compare algorithmic changes. Some of these become household names (at least, among households that train models!), such as MNIST, CIFAR-10, and ImageNet.\n",
    "\n",
    "The datasets used in this book have been selected because they provide great examples of the kinds of data that you are likely to encounter, and the academic literature has many examples of model results using these datasets to which you can compare your work.\n",
    "\n",
    "Most datasets used in this book took the creators a lot of work to build. For instance, later in the book we’ll be showing you how to create a model that can translate between French and English. The key input to this is a French/English parallel text corpus prepared back in 2009 by Professor Chris Callison-Burch of the University of Pennsylvania. This dataset contains over 20 million sentence pairs in French and English. He built the dataset in a really clever way: by crawling millions of Canadian web pages (which are often multilingual) and then using a set of simple heuristics to transform URLs of French content onto URLs pointing to the same content in English.\n",
    "\n",
    "As you look at datasets throughout this book, think about where they might have come from, and how they might have been curated. Then think about what kinds of interesting datasets you could create for your own projects. (We’ll even take you step by step through the process of creating your own image dataset soon.)\n",
    "\n",
    "fast.ai has spent a lot of time creating cut-down versions of popular datasets that are specially designed to support rapid prototyping and experimentation, and to be easier to learn with. In this book we will often start by using one of the cut-down versions and later scale up to the full-size version (just as we're doing in this chapter!). In fact, this is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and only use the full dataset when they have a good understanding of what they have to do.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9fc2e13995292ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation Sets and Test Sets\n",
    "\n",
    "Each of the models we trained showed a training and validation loss. A good validation set is one of the most important pieces of the training process\n",
    "\n",
    "If we train our model with all of our data, then evaluated the model using the same data, we won't be able to tell how well our model performed on data it hasn't seen\n",
    "\n",
    "To avoid this, we first split our dataset into two sets: training set (model sees in training) and the validation set (development set) which is only used for evaluation\n",
    "- Lets us test that the model learns lessons from the training data that generalize to new data -> validation data\n",
    "\n",
    "Splitting off our validation data means our model never sees it in training and so is completely untainted by it and is in no way cheating right?\n",
    "- NO, not necessarily. In realistic scenarios, we rarely build a model just by training its weight parameters once. We likely explore many versions of a model through various modelling choices like network architecture, learning rates, data augmentation strategies and other factors. Many of these choices are described as choices of *hyperparameters*\n",
    "- Hyperparameters -> parameters about parameters. They are the higher level choices that govern the meaning of the weight parameters\n",
    "\n",
    "The problem is that even though the ordinary training process is only looking at predictions on the training data when it learns values for the weight parameters, the same is not true of us. We, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values! So subsequent versions of the model are, indirectly, shaped by us having seen the validation data. Just as the automatic training process is in danger of overfitting the training data, we are in danger of overfitting the validation data through human trial and error and exploration.\n",
    "\n",
    "The solution is to introduce another level of even more highly reserved data -> test set\n",
    "Just as we hold back the validation data from the training process, we must hold back the test data set from even ourselves\n",
    "IT CANNOT BE USED TO IMPROVE THE MODEL, it can only be used to evaluate the model at the end of our efforts\n",
    "We define a hierarchy of cuts of our data, based on how fully we want to hide it from training and modeling processes.\n",
    "- Training data is fully exposed, validation data is less exposed, test data is totally hidden\n",
    "\n",
    "The test and validation sets should have enough data to ensure that you get a good estimate of your accuracy\n",
    "e.g. you want 30 cats in validation set for a cat detector\n",
    "So if you have thousands of items in dataset, using 20% validation set size may be more than you need\n",
    "\n",
    "Having two levels of \"reserved data\" - test set (totally hidden until final test) and validation set for tuning, may seem extreme. It is usually necessary as models tend to gravitate towards the simplest way to do good predictions (memorization) and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn't mean we always need a separate test set—if you have very little data, you may need to just have a validation set—but generally it's best to use one if at all possible.\n",
    "\n",
    "This same discipline can be critical if you intend to hire a third party to perform modeling work on your behalf. A third party might not understand your requirements accurately, or their incentives might even encourage them to misunderstand them. A good test set can greatly mitigate these risks and let you evaluate whether their work solves your actual problem.\n",
    "\n",
    "To put it bluntly, if you're a senior decision maker in your organization (or you're advising senior decision makers), the most important takeaway is this: if you ensure that you really understand what test and validation sets are and why they're important, then you'll avoid the single biggest source of failures we've seen when organizations decide to use AI. For instance, if you're considering bringing in an external vendor or service, make sure that you hold out some test data that the vendor never gets to see. Then you check their model on your test data, using a metric that you choose based on what actually matters to you in practice, and you decide what level of performance is adequate. (It's also a good idea for you to try out some simple baseline yourself, so you know what a really simple model can achieve. Often it'll turn out that your simple model performs just as well as one produced by an external \"expert\"!)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e467fc492871e86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Judgement in Defining Test Sets\n",
    "To define good validation set and test set, you will sometimes want to do more than just randomly grab a fraction of your original dataset\n",
    "Key property is that they have to representative of the new data you will see in the future\n",
    "Many example cases are from predictive modelling competitions on the Kaggle platform\n",
    "\n",
    "One case might be if you are looking at time series data. For time series, choosing a random subset of the data will be both too easy (look at the data both before and after the dates you are trying to predict) and not representative of most business use case (using historical data to build a model for use in future)\n",
    "If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (e.g. last two weeks or last month of available data)\n",
    "\n",
    "Use the earlier data as your training set, and the laster data for the validation set\n",
    "\n",
    "For example, Kaggle had a competition to predict the sales in a chain of Ecuadorian grocery stores. Kaggle's training data ran from Jan 1 2013 to Aug 15 2017, and the test data spanned Aug 16 2017 to Aug 31 2017. That way, the competition organizer ensured that entrants were making predictions for a time period that was in the future, from the perspective of their model. This is similar to the way quant hedge fund traders do back-testing to check whether their models are predictive of future periods, based on past data.\n",
    "\n",
    "\n",
    "\n",
    "A second common case is when you can easily anticipate ways the data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\n",
    "\n",
    "In the Kaggle distracted driver competition, the independent variables are pictures of drivers at the wheel of a car, and the dependent variables are categories such as texting, eating, or safely looking ahead. Lots of pictures are of the same drivers in different positions, as we can see in <<img_driver>>. If you were an insurance company building a model from this data, note that you would be most interested in how the model performs on drivers it hasn't seen before (since you would likely have training data only for a small group of people). In recognition of this, the test data for the competition consists of images of people that don't appear in the training set.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af7c923fc9f6f9a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
